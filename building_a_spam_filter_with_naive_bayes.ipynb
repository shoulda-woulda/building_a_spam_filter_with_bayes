{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Spam Filter with Naive Bayes\n",
    "\n",
    "#### The goal for this script is to create an SMS filtering system for classifying incoming SMS as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data, split training/test data and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_collection_data = pd.read_csv('data\\SMSSpamCollection', sep='\\t', header=None, names=['Label','SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_collection_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label    0\n",
       "SMS      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_collection_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_collection_data['Label'].value_counts() / len(sms_collection_data) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 5572 SMS messages with 2 columns, one with a label as to whether the message is spam or ham (not spam), and another with the SMS message body.\n",
    "\n",
    "Roughly 86.6% of SMS are ham, 13.4% are spam.\n",
    "\n",
    "There are no NaN values across the dataset.\n",
    "\n",
    "Splitting the dataset up into 80% for training the model and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has a shape of:  (4458, 2)\n",
      "Label\n",
      "ham     86.54105\n",
      "spam    13.45895\n",
      "Name: count, dtype: float64\n",
      "Test data has a shape of:  (1114, 2)\n",
      "Label\n",
      "ham     86.804309\n",
      "spam    13.195691\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sms_collection_data_randomized = sms_collection_data.sample(frac=1, random_state=1)\n",
    "\n",
    "training_test_index = round(len(sms_collection_data_randomized) * 0.8)\n",
    "\n",
    "training_data = sms_collection_data_randomized[:training_test_index].reset_index(drop=True)\n",
    "test_data = sms_collection_data_randomized[training_test_index:].reset_index(drop=True)\n",
    "\n",
    "print(\"Training data has a shape of: \", training_data.shape)\n",
    "print(training_data['Label'].value_counts() / len(training_data) * 100)\n",
    "\n",
    "print(\"Test data has a shape of: \", test_data.shape)\n",
    "print(test_data['Label'].value_counts() / len(test_data) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test samples have spam and ham percentages very close to our population.\n",
    "\n",
    "Cleaning the training dataset to remove punctuation and any upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['SMS'] = training_data['SMS'].replace(to_replace=r'\\W', value=' ', regex=True)\n",
    "\n",
    "training_data['SMS'] = training_data['SMS'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each unique word in the training data SMS messages must be turned into a header and counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_sms = []\n",
    "\n",
    "for message in training_data['SMS']:\n",
    "    list_of_words_in_sms = message.split()\n",
    "\n",
    "    for word in list_of_words_in_sms:\n",
    "        words_in_sms.append(word)\n",
    "\n",
    "unique_words_in_sms = list(set(words_in_sms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_unique_words = {unique_word: [0] * len(training_data['SMS']) for unique_word in unique_words_in_sms}\n",
    "\n",
    "for index, message in enumerate(training_data['SMS']):\n",
    "    for word in message.split():\n",
    "        count_of_unique_words[word][index] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brighten</th>\n",
       "      <th>frosty</th>\n",
       "      <th>suply</th>\n",
       "      <th>bridgwater</th>\n",
       "      <th>gave</th>\n",
       "      <th>pax</th>\n",
       "      <th>lighters</th>\n",
       "      <th>enters</th>\n",
       "      <th>se</th>\n",
       "      <th>82242</th>\n",
       "      <th>...</th>\n",
       "      <th>tarot</th>\n",
       "      <th>bbd</th>\n",
       "      <th>pooja</th>\n",
       "      <th>1450</th>\n",
       "      <th>prepared</th>\n",
       "      <th>wont</th>\n",
       "      <th>checked</th>\n",
       "      <th>infront</th>\n",
       "      <th>kb</th>\n",
       "      <th>reach</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   brighten  frosty  suply  bridgwater  gave  pax  lighters  enters  se  \\\n",
       "0         0       0      0           0     0    0         0       0   0   \n",
       "1         0       0      0           0     0    0         0       0   0   \n",
       "2         0       0      0           0     0    0         0       0   0   \n",
       "3         0       0      0           0     0    0         0       0   0   \n",
       "4         0       0      0           0     0    0         0       0   0   \n",
       "\n",
       "   82242  ...  tarot  bbd  pooja  1450  prepared  wont  checked  infront  kb  \\\n",
       "0      0  ...      0    0      0     0         0     0        0        0   0   \n",
       "1      0  ...      0    0      0     0         0     0        0        0   0   \n",
       "2      0  ...      0    0      0     0         0     0        0        0   0   \n",
       "3      0  ...      0    0      0     0         0     0        0        0   0   \n",
       "4      0  ...      0    0      0     0         0     0        0        0   0   \n",
       "\n",
       "   reach  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 7783 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = pd.DataFrame(count_of_unique_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>brighten</th>\n",
       "      <th>frosty</th>\n",
       "      <th>suply</th>\n",
       "      <th>bridgwater</th>\n",
       "      <th>gave</th>\n",
       "      <th>pax</th>\n",
       "      <th>lighters</th>\n",
       "      <th>enters</th>\n",
       "      <th>...</th>\n",
       "      <th>tarot</th>\n",
       "      <th>bbd</th>\n",
       "      <th>pooja</th>\n",
       "      <th>1450</th>\n",
       "      <th>prepared</th>\n",
       "      <th>wont</th>\n",
       "      <th>checked</th>\n",
       "      <th>infront</th>\n",
       "      <th>kb</th>\n",
       "      <th>reach</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  brighten  frosty  \\\n",
       "0   ham                       yep  by the pretty sculpture         0       0   \n",
       "1   ham      yes  princess  are you going to make me moan          0       0   \n",
       "2   ham                         welp apparently he retired         0       0   \n",
       "3   ham                                            havent          0       0   \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ...         0       0   \n",
       "\n",
       "   suply  bridgwater  gave  pax  lighters  enters  ...  tarot  bbd  pooja  \\\n",
       "0      0           0     0    0         0       0  ...      0    0      0   \n",
       "1      0           0     0    0         0       0  ...      0    0      0   \n",
       "2      0           0     0    0         0       0  ...      0    0      0   \n",
       "3      0           0     0    0         0       0  ...      0    0      0   \n",
       "4      0           0     0    0         0       0  ...      0    0      0   \n",
       "\n",
       "   1450  prepared  wont  checked  infront  kb  reach  \n",
       "0     0         0     0        0        0   0      0  \n",
       "1     0         0     0        0        0   0      0  \n",
       "2     0         0     0        0        0   0      0  \n",
       "3     0         0     0        0        0   0      0  \n",
       "4     0         0     0        0        0   0      0  \n",
       "\n",
       "[5 rows x 7785 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_clean = pd.concat([training_data, word_counts], axis=1)\n",
    "training_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Calculate Probabilities\n",
    "\n",
    "These are the formulae for Naive Bayes algorithm that will be used to calculate the probabilities of an SMS being spam from the words it contains.\n",
    "\n",
    "$$ P\n",
    "(\n",
    "S\n",
    "p\n",
    "a\n",
    "m\n",
    "|\n",
    "w\n",
    "1\n",
    ",\n",
    "w\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "w\n",
    "n\n",
    ")\n",
    "∝\n",
    "P\n",
    "(\n",
    "S\n",
    "p\n",
    "a\n",
    "m\n",
    ")\n",
    "⋅\n",
    "n\n",
    "∏\n",
    "i\n",
    "=\n",
    "1\n",
    " \n",
    "P\n",
    "(\n",
    "w\n",
    "i\n",
    "|\n",
    "S\n",
    "p\n",
    "a\n",
    "m\n",
    ")\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P\n",
    "(\n",
    "H\n",
    "a\n",
    "m\n",
    "|\n",
    "w\n",
    "1\n",
    ",\n",
    "w\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "w\n",
    "n\n",
    ")\n",
    "∝\n",
    "P\n",
    "(\n",
    "H\n",
    "a\n",
    "m\n",
    ")\n",
    "⋅\n",
    "n\n",
    "∏\n",
    "i\n",
    "=\n",
    "1\n",
    " \n",
    "P\n",
    "(\n",
    "w\n",
    "i\n",
    "|\n",
    "H\n",
    "a\n",
    "m\n",
    ") $$\n",
    "\n",
    "\n",
    "To calculate the respective weight of each word as spam and ham, these formulae need to be used:\n",
    "\n",
    "$$P\n",
    "(\n",
    "w\n",
    "i\n",
    "|\n",
    "S\n",
    "p\n",
    "a\n",
    "m\n",
    ")\n",
    "=\n",
    "N\n",
    "w\n",
    "i\n",
    "|\n",
    "S\n",
    "p\n",
    "a\n",
    "m\n",
    "+\n",
    "α\n",
    "N\n",
    "S\n",
    "p\n",
    "a\n",
    "m\n",
    "+\n",
    "α\n",
    "⋅\n",
    "N\n",
    "V\n",
    "o\n",
    "c\n",
    "a\n",
    "b\n",
    "u\n",
    "l\n",
    "a\n",
    "r\n",
    "y\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P\n",
    "(\n",
    "w\n",
    "i\n",
    "|\n",
    "H\n",
    "a\n",
    "m\n",
    ")\n",
    "=\n",
    "N\n",
    "w\n",
    "i\n",
    "|\n",
    "H\n",
    "a\n",
    "m\n",
    "+\n",
    "α\n",
    "N\n",
    "H\n",
    "a\n",
    "m\n",
    "+\n",
    "α\n",
    "⋅\n",
    "N\n",
    "V\n",
    "o\n",
    "c\n",
    "a\n",
    "b\n",
    "u\n",
    "l\n",
    "a\n",
    "r\n",
    "y\n",
    "$$\n",
    "\n",
    "Some of the terms are constant for every new message. These will be calculated now.\n",
    "\n",
    "α = 1 will be used for Laplace smoothing as there are a high number of unique words and a fairly small dataset so the probabilties of limited or no occurances is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_spam = training_data[training_data['Label']=='spam']\n",
    "only_ham = training_data[training_data['Label']=='ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13458950201884254"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_spam = only_spam['Label'].count()/training_data['Label'].count()\n",
    "\n",
    "p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8654104979811574"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ham = only_ham['Label'].count()/training_data['Label'].count()\n",
    "\n",
    "p_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15190"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_spam = 0\n",
    "\n",
    "for message in only_spam['SMS']:\n",
    "    n_spam += (len(message.split()))\n",
    "\n",
    "n_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57237"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ham = 0\n",
    "\n",
    "for message in only_ham['SMS']:\n",
    "    n_ham += (len(message.split()))\n",
    "\n",
    "n_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_volcab = len(count_of_unique_words)\n",
    "\n",
    "n_volcab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_w_spam_parameters = {unique_word: 0 for unique_word in unique_words_in_sms}\n",
    "\n",
    "p_w_ham_parameters = {unique_word: 0 for unique_word in unique_words_in_sms}\n",
    "\n",
    "## Creating counts of each unique word in SMS that are marked as spam\n",
    "for message in only_spam['SMS']:\n",
    "    list_of_words = message.split()\n",
    "\n",
    "    for word in list_of_words:\n",
    "        p_w_spam_parameters[word] +=1\n",
    "\n",
    "## Creating the probabilities of each unique word in SMS appearing in spam.\n",
    "\n",
    "for key, value in p_w_spam_parameters.items():\n",
    "    p_w_spam_parameters[key] = (value + alpha)/(n_spam + alpha * n_volcab)\n",
    "\n",
    "## Creating counts of each unique word in SMS that are marked as ham\n",
    "for message in only_ham['SMS']:\n",
    "    list_of_words = message.split()\n",
    "\n",
    "    for word in list_of_words:\n",
    "        p_w_ham_parameters[word] +=1    \n",
    "\n",
    "## Creating the probabilities of each unique word in SMS appearing in ham.\n",
    "\n",
    "for key, value in p_w_ham_parameters.items():\n",
    "    p_w_ham_parameters[key] = (value + alpha)/(n_ham + alpha * n_volcab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Classifying a New SMS\n",
    "\n",
    "All parameters are calculated so a spam filter can now be made. The spam filter will constitute a function that:\n",
    "\n",
    "- Takes in as input a new message (w1, w2, ..., wn)\n",
    "- Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n",
    "- Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "\n",
    "    -If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham\n",
    "\n",
    "    -If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam\n",
    "    \n",
    "    -If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "    ## message words have been split into list\n",
    "    spam_word_probabilities = 1\n",
    "    for word in message:\n",
    "        if word in p_w_spam_parameters:\n",
    "            spam_word_probabilities *= p_w_spam_parameters[word]\n",
    "    \n",
    "    p_spam_given_message = p_spam* spam_word_probabilities\n",
    "\n",
    "    ham_word_probabilities = 1\n",
    "    for word in message:\n",
    "        if word in p_w_ham_parameters:\n",
    "            ham_word_probabilities *= p_w_ham_parameters[word]\n",
    "\n",
    "    p_ham_given_message = p_ham* ham_word_probabilities\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    \n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    \n",
    "    else:\n",
    "        print('Equal probabilities, flag')\n",
    "        return 'equal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('Click the link now to claim your free prize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('Hi mate just nipping down for a couple. Want to join?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Evaluating the Performance of the Filter\n",
    "\n",
    "The test dataset of 1114 SMS will now be used against the filter to calculate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1114, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal probabilities, flag\n"
     ]
    }
   ],
   "source": [
    "test_data['spam_or_ham_filter'] = test_data['SMS'].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The simple filter worked with a certain level of accuracy of 98.74% against the test data provided.\n",
      "This equates to 1100 correct classifications out of a total of 1114 messages.\n",
      "\n",
      "Further investigations could include looking at the messages that were mis-classified.\n",
      "Increasing the volume of training data could also be considered.\n"
     ]
    }
   ],
   "source": [
    "count_correct_labels = 0\n",
    "\n",
    "total = test_data.shape[0]\n",
    "    \n",
    "for row in test_data.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['spam_or_ham_filter']:\n",
    "        count_correct_labels += 1\n",
    "\n",
    "percentage_correct = round(count_correct_labels/total*100,2)\n",
    "\n",
    "print(f\"The simple filter worked with a certain level of accuracy of {percentage_correct}% against the test data provided.\\nThis equates to {count_correct_labels} correct classifications out of a total of {total} messages.\\n\\nFurther investigations could include looking at the messages that were mis-classified.\\nIncreasing the volume of training data could also be considered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Evaluating increasing training data by 10%\n",
    "\n",
    "As a quick validation excercise I wonder what increasing our training data volume from 80% of our total data to 90% would do to the accuracy of the filter (if any).\n",
    "\n",
    "_n.b. increasing training data will reduce our testing data and this must be considered if comparing accuracy._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has a shape of:  (5015, 2)\n",
      "Label\n",
      "ham     86.66002\n",
      "spam    13.33998\n",
      "Name: count, dtype: float64\n",
      "Test data has a shape of:  (557, 2)\n",
      "Label\n",
      "ham     85.996409\n",
      "spam    14.003591\n",
      "Name: count, dtype: float64\n",
      "The simple filter worked with a certain level of accuracy of 99.28% against the test data provided.\n",
      "This equates to 553 correct classifications out of a total of 557 messages.\n"
     ]
    }
   ],
   "source": [
    "sms_collection_data_randomized = sms_collection_data.sample(frac=1, random_state=1)\n",
    "\n",
    "training_test_index = round(len(sms_collection_data_randomized) * 0.9)\n",
    "\n",
    "training_data = sms_collection_data_randomized[:training_test_index].reset_index(drop=True)\n",
    "test_data = sms_collection_data_randomized[training_test_index:].reset_index(drop=True)\n",
    "\n",
    "print(\"Training data has a shape of: \", training_data.shape)\n",
    "print(training_data['Label'].value_counts() / len(training_data) * 100)\n",
    "\n",
    "print(\"Test data has a shape of: \", test_data.shape)\n",
    "print(test_data['Label'].value_counts() / len(test_data) * 100)\n",
    "\n",
    "training_data['SMS'] = training_data['SMS'].replace(to_replace=r'\\W', value=' ', regex=True)\n",
    "\n",
    "training_data['SMS'] = training_data['SMS'].str.lower()\n",
    "\n",
    "words_in_sms = []\n",
    "\n",
    "for message in training_data['SMS']:\n",
    "    list_of_words_in_sms = message.split()\n",
    "\n",
    "    for word in list_of_words_in_sms:\n",
    "        words_in_sms.append(word)\n",
    "\n",
    "unique_words_in_sms = list(set(words_in_sms))\n",
    "\n",
    "count_of_unique_words = {unique_word: [0] * len(training_data['SMS']) for unique_word in unique_words_in_sms}\n",
    "\n",
    "for index, message in enumerate(training_data['SMS']):\n",
    "    for word in message.split():\n",
    "        count_of_unique_words[word][index] += 1\n",
    "word_counts = pd.DataFrame(count_of_unique_words)\n",
    "training_data_clean = pd.concat([training_data, word_counts], axis=1)\n",
    "training_data_clean.head()\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "only_spam = training_data[training_data['Label']=='spam']\n",
    "only_ham = training_data[training_data['Label']=='ham']\n",
    "\n",
    "only_spam = training_data[training_data['Label']=='spam']\n",
    "only_ham = training_data[training_data['Label']=='ham']\n",
    "\n",
    "only_spam = training_data[training_data['Label']=='spam']\n",
    "only_ham = training_data[training_data['Label']=='ham']\n",
    "\n",
    "p_spam = only_spam['Label'].count()/training_data['Label'].count()\n",
    "\n",
    "p_ham = only_ham['Label'].count()/training_data['Label'].count()\n",
    "\n",
    "n_spam = 0\n",
    "\n",
    "for message in only_spam['SMS']:\n",
    "    n_spam += (len(message.split()))\n",
    "\n",
    "n_ham = 0\n",
    "\n",
    "for message in only_ham['SMS']:\n",
    "    n_ham += (len(message.split()))\n",
    "\n",
    "n_volcab = len(count_of_unique_words)\n",
    "\n",
    "p_w_spam_parameters = {unique_word: 0 for unique_word in unique_words_in_sms}\n",
    "\n",
    "p_w_ham_parameters = {unique_word: 0 for unique_word in unique_words_in_sms}\n",
    "\n",
    "## Creating counts of each unique word in SMS that are marked as spam\n",
    "for message in only_spam['SMS']:\n",
    "    list_of_words = message.split()\n",
    "\n",
    "    for word in list_of_words:\n",
    "        p_w_spam_parameters[word] +=1\n",
    "\n",
    "## Creating the probabilities of each unique word in SMS appearing in spam.\n",
    "\n",
    "for key, value in p_w_spam_parameters.items():\n",
    "    p_w_spam_parameters[key] = (value + alpha)/(n_spam + alpha * n_volcab)\n",
    "\n",
    "## Creating counts of each unique word in SMS that are marked as ham\n",
    "for message in only_ham['SMS']:\n",
    "    list_of_words = message.split()\n",
    "\n",
    "    for word in list_of_words:\n",
    "        p_w_ham_parameters[word] +=1    \n",
    "\n",
    "## Creating the probabilities of each unique word in SMS appearing in ham.\n",
    "\n",
    "for key, value in p_w_ham_parameters.items():\n",
    "    p_w_ham_parameters[key] = (value + alpha)/(n_ham + alpha * n_volcab)\n",
    "\n",
    "test_data['spam_or_ham_filter'] = test_data['SMS'].apply(classify)\n",
    "\n",
    "count_correct_labels = 0\n",
    "\n",
    "total = test_data.shape[0]\n",
    "    \n",
    "for row in test_data.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['spam_or_ham_filter']:\n",
    "        count_correct_labels += 1\n",
    "\n",
    "percentage_correct = round(count_correct_labels/total*100,2)\n",
    "\n",
    "print(f\"The simple filter worked with a certain level of accuracy of {percentage_correct}% against the test data provided.\\nThis equates to {count_correct_labels} correct classifications out of a total of {total} messages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, increasing training data resulted in a higher accuracy filter. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
